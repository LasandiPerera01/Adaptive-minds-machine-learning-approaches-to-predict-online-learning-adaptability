{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9104b73b-55c8-440f-8c60-c4e9bb1c6376",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9104b73b-55c8-440f-8c60-c4e9bb1c6376",
        "outputId": "e2bee873-81b0-40f4-8926-93f1f22d624b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a1159569",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Dataset Overview ===\n",
            "Rows: 1205, Columns: 14\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# 1Ô∏è‚É£ Load the dataset\n",
        "# ==============================================================\n",
        "df = pd.read_csv(\"students_adaptability_level_online_education.csv\")\n",
        "\n",
        "print(\"=== Dataset Overview ===\")\n",
        "print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
        "print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f83212c0",
      "metadata": {},
      "source": [
        "#### Basic Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6aa46fc4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Missing Values Check ===\n",
            "‚úÖ No missing values found.\n",
            "\n",
            "=== Duplicate Rows Check ===\n",
            "Total Duplicates Found: 949\n",
            "‚ö†Ô∏è Duplicates exist but will NOT be removed (kept for representation).\n",
            "\n",
            "‚úÖ No numeric columns available for outlier detection.\n",
            "\n",
            "‚úÖ Hybrid Encoding Complete (Ordinal + Label Encoding).\n",
            "\n",
            "=== Encoding Mappings for Each Feature ===\n",
            "\n",
            "Gender:\n",
            "  Boy ‚Üí 0\n",
            "  Girl ‚Üí 1\n",
            "\n",
            "Institution Type:\n",
            "  Government ‚Üí 0\n",
            "  Non Government ‚Üí 1\n",
            "\n",
            "IT Student:\n",
            "  No ‚Üí 0\n",
            "  Yes ‚Üí 1\n",
            "\n",
            "Location:\n",
            "  No ‚Üí 0\n",
            "  Yes ‚Üí 1\n",
            "\n",
            "Internet Type:\n",
            "  Mobile Data ‚Üí 0\n",
            "  Wifi ‚Üí 1\n",
            "\n",
            "Self Lms:\n",
            "  No ‚Üí 0\n",
            "  Yes ‚Üí 1\n",
            "\n",
            "Device:\n",
            "  Computer ‚Üí 0\n",
            "  Mobile ‚Üí 1\n",
            "  Tab ‚Üí 2\n",
            "\n",
            "Age:\n",
            "  1-5 ‚Üí 0\n",
            "  6-10 ‚Üí 1\n",
            "  11-15 ‚Üí 2\n",
            "  16-20 ‚Üí 3\n",
            "  21-25 ‚Üí 4\n",
            "  26-30 ‚Üí 5\n",
            "\n",
            "Education Level:\n",
            "  School ‚Üí 0\n",
            "  College ‚Üí 1\n",
            "  University ‚Üí 2\n",
            "\n",
            "Load-shedding:\n",
            "  Low ‚Üí 0\n",
            "  High ‚Üí 1\n",
            "\n",
            "Financial Condition:\n",
            "  Poor ‚Üí 0\n",
            "  Mid ‚Üí 1\n",
            "  Rich ‚Üí 2\n",
            "\n",
            "Network Type:\n",
            "  2G ‚Üí 0\n",
            "  3G ‚Üí 1\n",
            "  4G ‚Üí 2\n",
            "\n",
            "Class Duration:\n",
            "  0 ‚Üí 0\n",
            "  1-3 ‚Üí 1\n",
            "  3-6 ‚Üí 2\n",
            "\n",
            "Adaptivity Level:\n",
            "  Low ‚Üí 0\n",
            "  Moderate ‚Üí 1\n",
            "  High ‚Üí 2\n",
            "\n",
            "‚úÖ Data split into training and test sets.\n",
            "Training set: 964 rows\n",
            "Testing set: 241 rows\n",
            "\n",
            "=== Class Distribution (Before SMOTE) ===\n",
            "Adaptivity Level\n",
            "1.0    500\n",
            "0.0    384\n",
            "2.0     80\n",
            "Name: count, dtype: int64\n",
            "=== Class Distribution (After SMOTE) ===\n",
            "Adaptivity Level\n",
            "1.0    500\n",
            "0.0    500\n",
            "2.0    500\n",
            "Name: count, dtype: int64\n",
            "\n",
            "‚ÑπÔ∏è Skipping standardization: categorical-encoded values should retain meaning for EDA & model interpretation.\n",
            "\n",
            "Final Shapes:\n",
            "Training features: (1500, 13)\n",
            "Testing features:  (241, 13)\n",
            "\n",
            "‚úÖ Preprocessing completed successfully with Hybrid Encoding.\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# 2Ô∏è‚É£ Check for missing values\n",
        "# ==============================================================\n",
        "print(\"=== Missing Values Check ===\")\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values[missing_values > 0] if missing_values.sum() > 0 else \"‚úÖ No missing values found.\")\n",
        "print()\n",
        "\n",
        "# ==============================================================\n",
        "# 3Ô∏è‚É£ Check for duplicates (do NOT remove them)\n",
        "# ==============================================================\n",
        "duplicates_count = df.duplicated().sum()\n",
        "print(f\"=== Duplicate Rows Check ===\\nTotal Duplicates Found: {duplicates_count}\")\n",
        "if duplicates_count > 0:\n",
        "    print(\"‚ö†Ô∏è Duplicates exist but will NOT be removed (kept for representation).\")\n",
        "print()\n",
        "\n",
        "# ==============================================================\n",
        "# 4Ô∏è‚É£ Outlier Detection (for numeric columns only)\n",
        "# ==============================================================\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "if numeric_cols:\n",
        "    print(\"=== Outlier Detection (IQR method) ===\")\n",
        "    for col in numeric_cols:\n",
        "        Q1, Q3 = df[col].quantile([0.25, 0.75])\n",
        "        IQR = Q3 - Q1\n",
        "        lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
        "        outliers = df[(df[col] < lower) | (df[col] > upper)]\n",
        "        print(f\"{col}: {len(outliers)} outliers\")\n",
        "else:\n",
        "    print(\"‚úÖ No numeric columns available for outlier detection.\")\n",
        "print()\n",
        "\n",
        "# ==============================================================\n",
        "# 5Ô∏è‚É£ Hybrid Encoding: Ordinal + Label Encoding\n",
        "# ==============================================================\n",
        "\n",
        "df_encoded = df.copy()\n",
        "\n",
        "# --- Define ordinal features with meaningful order ---\n",
        "ordinal_mappings = {\n",
        "    'Age': ['1-5', '6-10', '11-15', '16-20', '21-25', '26-30'],\n",
        "    'Education Level': ['School', 'College', 'University'],\n",
        "    'Load-shedding': ['Low', 'High'],\n",
        "    'Financial Condition': ['Poor', 'Mid', 'Rich'],\n",
        "    'Network Type': ['2G', '3G', '4G'],\n",
        "    'Class Duration': ['0', '1-3', '3-6'],\n",
        "    'Adaptivity Level': ['Low', 'Moderate', 'High']  # Target variable\n",
        "}\n",
        "\n",
        "ordinal_features = list(ordinal_mappings.keys())\n",
        "nominal_features = [col for col in df.columns if col not in ordinal_features]\n",
        "\n",
        "# --- Apply Ordinal Encoding ---\n",
        "ordinal_encoder = OrdinalEncoder(categories=[ordinal_mappings[col] for col in ordinal_features])\n",
        "df_encoded[ordinal_features] = ordinal_encoder.fit_transform(df_encoded[ordinal_features])\n",
        "\n",
        "# --- Apply Label Encoding for Nominal Features ---\n",
        "label_encoders = {}\n",
        "encoding_mappings = {}\n",
        "\n",
        "for col in nominal_features:\n",
        "    le = LabelEncoder()\n",
        "    df_encoded[col] = le.fit_transform(df_encoded[col])\n",
        "    label_encoders[col] = le\n",
        "    encoding_mappings[col] = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "\n",
        "# --- Record Ordinal mappings ---\n",
        "for col in ordinal_mappings:\n",
        "    encoding_mappings[col] = {cat: i for i, cat in enumerate(ordinal_mappings[col])}\n",
        "\n",
        "print(\"‚úÖ Hybrid Encoding Complete (Ordinal + Label Encoding).\\n\")\n",
        "\n",
        "# Show encoding interpretation\n",
        "print(\"=== Encoding Mappings for Each Feature ===\")\n",
        "for col, mapping in encoding_mappings.items():\n",
        "    print(f\"\\n{col}:\")\n",
        "    for k, v in mapping.items():\n",
        "        print(f\"  {k} ‚Üí {v}\")\n",
        "\n",
        "# ==============================================================\n",
        "# 6Ô∏è‚É£ Split into Training and Testing Sets\n",
        "# ==============================================================\n",
        "X = df_encoded.drop(columns=['Adaptivity Level'])\n",
        "y = df_encoded['Adaptivity Level']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Data split into training and test sets.\")\n",
        "print(f\"Training set: {X_train.shape[0]} rows\")\n",
        "print(f\"Testing set: {X_test.shape[0]} rows\\n\")\n",
        "\n",
        "# ==============================================================\n",
        "# 7Ô∏è‚É£ Check class imbalance (training data only)\n",
        "# ==============================================================\n",
        "print(\"=== Class Distribution (Before SMOTE) ===\")\n",
        "print(y_train.value_counts())\n",
        "\n",
        "\n",
        "# ==============================================================\n",
        "# 8Ô∏è‚É£ Apply SMOTE only on training data\n",
        "# ==============================================================\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"=== Class Distribution (After SMOTE) ===\")\n",
        "print(pd.Series(y_train_resampled).value_counts())\n",
        "\n",
        "\n",
        "# ==============================================================\n",
        "# 9Ô∏è‚É£ (No Standardization applied)\n",
        "# ==============================================================\n",
        "print(\"\\n‚ÑπÔ∏è Skipping standardization: categorical-encoded values should retain meaning for EDA & model interpretation.\")\n",
        "\n",
        "# ==============================================================\n",
        "# üîü Final Checks\n",
        "# ==============================================================\n",
        "print(\"\\nFinal Shapes:\")\n",
        "print(f\"Training features: {X_train_resampled.shape}\")\n",
        "print(f\"Testing features:  {X_test.shape}\")\n",
        "\n",
        "print(\"\\n‚úÖ Preprocessing completed successfully with Hybrid Encoding.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8814e988",
      "metadata": {},
      "source": [
        "#### Advanced Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ZKzk9_jncWvS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKzk9_jncWvS",
        "outputId": "21ed7d53-b02b-4f80-d7a0-eeed950fbc00"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Simple MLP =====\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.77      0.75        96\n",
            "         1.0       0.76      0.72      0.74       125\n",
            "         2.0       0.52      0.55      0.54        20\n",
            "\n",
            "    accuracy                           0.73       241\n",
            "   macro avg       0.67      0.68      0.67       241\n",
            "weighted avg       0.73      0.73      0.73       241\n",
            "\n",
            "\n",
            "===== Deep MLP =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      0.94      0.93        96\n",
            "         1.0       0.96      0.87      0.91       125\n",
            "         2.0       0.67      1.00      0.80        20\n",
            "\n",
            "    accuracy                           0.91       241\n",
            "   macro avg       0.85      0.94      0.88       241\n",
            "weighted avg       0.92      0.91      0.91       241\n",
            "\n",
            "\n",
            "===== Regularized MLP =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7920381b9800> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.92      0.82        96\n",
            "         1.0       0.89      0.70      0.78       125\n",
            "         2.0       0.64      0.80      0.71        20\n",
            "\n",
            "    accuracy                           0.79       241\n",
            "   macro avg       0.76      0.80      0.77       241\n",
            "weighted avg       0.81      0.79      0.79       241\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7920381c55b0>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Function to build and evaluate a model\n",
        "def build_and_evaluate_nn(model_name, model):\n",
        "    print(f\"\\n===== {model_name} =====\")\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    history = model.fit(X_train_resampled, y_train_resampled, epochs=50, batch_size=16, verbose=0, validation_split=0.2)\n",
        "\n",
        "    # Evaluation\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = y_pred.argmax(axis=1)\n",
        "    print(classification_report(y_test, y_pred_classes))\n",
        "    return history\n",
        "\n",
        "# 1Ô∏è‚É£ Simple MLP\n",
        "model1 = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train_resampled.shape[1],)),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "build_and_evaluate_nn(\"Simple MLP\", model1)\n",
        "\n",
        "# 2Ô∏è‚É£ Deep MLP\n",
        "model2 = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_resampled.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "build_and_evaluate_nn(\"Deep MLP\", model2)\n",
        "\n",
        "# 3Ô∏è‚É£ Regularized MLP (Dropout + L2)\n",
        "from tensorflow.keras import regularizers\n",
        "model3 = Sequential([\n",
        "    Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape=(X_train_resampled.shape[1],)),\n",
        "    Dropout(0.4),\n",
        "    Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "    Dropout(0.3),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "build_and_evaluate_nn(\"Regularized MLP\", model3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d87e7719",
      "metadata": {},
      "source": [
        "##### Support Vector Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c347b01e-22e9-453c-bf77-375cbeee14c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c347b01e-22e9-453c-bf77-375cbeee14c5",
        "outputId": "c409ff76-4be7-4287-c588-49a4fd830469"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Linear SVM =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.64      0.78      0.70        96\n",
            "         1.0       0.75      0.44      0.56       125\n",
            "         2.0       0.20      0.50      0.29        20\n",
            "\n",
            "    accuracy                           0.58       241\n",
            "   macro avg       0.53      0.57      0.51       241\n",
            "weighted avg       0.66      0.58      0.59       241\n",
            "\n",
            "\n",
            "===== RBF SVM =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      0.80      0.74        96\n",
            "         1.0       0.82      0.58      0.68       125\n",
            "         2.0       0.36      0.75      0.48        20\n",
            "\n",
            "    accuracy                           0.68       241\n",
            "   macro avg       0.62      0.71      0.63       241\n",
            "weighted avg       0.73      0.68      0.69       241\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Linear SVM\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train_resampled, y_train_resampled)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "print(\"\\n===== Linear SVM =====\")\n",
        "print(classification_report(y_test, y_pred_linear))\n",
        "\n",
        "# RBF SVM\n",
        "svm_rbf = SVC(kernel='rbf', gamma='scale')\n",
        "svm_rbf.fit(X_train_resampled, y_train_resampled)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "print(\"\\n===== RBF SVM =====\")\n",
        "print(classification_report(y_test, y_pred_rbf))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AYJyplvtwr4R",
      "metadata": {
        "id": "AYJyplvtwr4R"
      },
      "source": [
        "Improve the Existing SVMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1AdQ5HizwrA9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AdQ5HizwrA9",
        "outputId": "80a3be9a-2443-4f52-974f-3e4400c6c474"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best Parameters: {'C': 100, 'gamma': 1, 'kernel': 'rbf'}\n",
            "Best CV Accuracy: 0.9033333333333333\n",
            "\n",
            "===== Tuned RBF SVM =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      0.94      0.93        96\n",
            "         1.0       0.94      0.87      0.90       125\n",
            "         2.0       0.64      0.90      0.75        20\n",
            "\n",
            "    accuracy                           0.90       241\n",
            "   macro avg       0.84      0.90      0.86       241\n",
            "weighted avg       0.91      0.90      0.90       241\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.01, 0.1, 1, 'scale', 'auto'],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(SVC(), param_grid, refit=True, cv=5, verbose=1)\n",
        "grid.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best CV Accuracy:\", grid.best_score_)\n",
        "\n",
        "# Evaluate on test set\n",
        "best_svm = grid.best_estimator_\n",
        "y_pred_best = best_svm.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"\\n===== Tuned RBF SVM =====\")\n",
        "print(classification_report(y_test, y_pred_best))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EdBFZH2Ww7Bb",
      "metadata": {
        "id": "EdBFZH2Ww7Bb"
      },
      "source": [
        "Try Different SVM Variants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nE2zR_Q5w-kw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nE2zR_Q5w-kw",
        "outputId": "40ade5f7-53fa-4de4-cc26-216defc8e0a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Polynomial SVM =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      0.85      0.77        96\n",
            "         1.0       0.83      0.64      0.72       125\n",
            "         2.0       0.45      0.65      0.53        20\n",
            "\n",
            "    accuracy                           0.73       241\n",
            "   macro avg       0.66      0.71      0.68       241\n",
            "weighted avg       0.75      0.73      0.73       241\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Polynomial Kernel SVM\n",
        "\n",
        "svm_poly = SVC(kernel='poly', degree=3, C=1)\n",
        "svm_poly.fit(X_train_resampled, y_train_resampled)\n",
        "y_pred_poly = svm_poly.predict(X_test)\n",
        "\n",
        "print(\"\\n===== Polynomial SVM =====\")\n",
        "print(classification_report(y_test, y_pred_poly))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bguhG0zjxGm_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bguhG0zjxGm_",
        "outputId": "a50e27b4-7cad-46a0-9883-167615f0c941"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Sigmoid SVM =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.36      0.47      0.41        96\n",
            "         1.0       0.71      0.04      0.08       125\n",
            "         2.0       0.06      0.35      0.11        20\n",
            "\n",
            "    accuracy                           0.24       241\n",
            "   macro avg       0.38      0.29      0.20       241\n",
            "weighted avg       0.52      0.24      0.21       241\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Sigmoid Kernel SVM\n",
        "\n",
        "svm_sigmoid = SVC(kernel='sigmoid', C=1)\n",
        "svm_sigmoid.fit(X_train_resampled, y_train_resampled)\n",
        "y_pred_sigmoid = svm_sigmoid.predict(X_test)\n",
        "\n",
        "print(\"\\n===== Sigmoid SVM =====\")\n",
        "print(classification_report(y_test, y_pred_sigmoid))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oum96R6bxRit",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oum96R6bxRit",
        "outputId": "64774684-65a2-4995-fa1a-d38ebb906409"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== LinearSVC (fast) =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.65      0.76      0.70        96\n",
            "         1.0       0.70      0.43      0.53       125\n",
            "         2.0       0.19      0.50      0.28        20\n",
            "\n",
            "    accuracy                           0.57       241\n",
            "   macro avg       0.52      0.56      0.50       241\n",
            "weighted avg       0.64      0.57      0.58       241\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#LinearSVC (Faster Linear Version)\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "linear_svm_fast = LinearSVC(C=1, max_iter=10000)\n",
        "linear_svm_fast.fit(X_train_resampled, y_train_resampled)\n",
        "y_pred_fast = linear_svm_fast.predict(X_test)\n",
        "\n",
        "print(\"\\n===== LinearSVC (fast) =====\")\n",
        "print(classification_report(y_test, y_pred_fast))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
